{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST 데이터 받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from six.moves import urllib\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "SOURCE_URL = 'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
    "FILENAME = SOURCE_URL.split('/')[-1]\n",
    "DATA_DIR = './datasets'\n",
    "\n",
    "def maybe_download(data_dir):\n",
    "    filepath = os.path.join(data_dir, FILENAME)\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    if not os.path.isfile(filepath):\n",
    "        def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write('\\r>> Downloading {} {:.1f} %'.format(\n",
    "                FILENAME, float(count * block_size) / float(total_size) * 100.0))\n",
    "            sys.stdout.flush()\n",
    "        filepath, _ = urllib.request.urlretrieve(SOURCE_URL, filepath, _progress)\n",
    "        print()\n",
    "        statinfo = os.stat(filepath)\n",
    "        print('Successfully donloaded', FILENAME, statinfo.st_size, 'bytes.')\n",
    "\n",
    "def load(data_dir, subset='train'):\n",
    "    maybe_download(data_dir)\n",
    "    filepath = os.path.join(data_dir, FILENAME)\n",
    "    \n",
    "    f = gzip.open(filepath, 'rb')\n",
    "    u = pickle._Unpickler(f)\n",
    "    u.encoding = 'latin1'\n",
    "    train_set, valid_set, test_set = u.load()\n",
    "    f.close()\n",
    "    \n",
    "    if subset == 'train':\n",
    "        trainx, trainy = train_set\n",
    "        trainx = trainx.astype(np.float32).reshape(trainx.shape[0], 28, 28)\n",
    "        trainy = trainy.astype(np.uint8)\n",
    "        return trainx, trainy\n",
    "    elif subset == 'test':\n",
    "        testx, testy = test_set\n",
    "        testx = testx.astype(np.float32).reshape(testx.shape[0], 28, 28)\n",
    "        testy = testy.astype(np.uint8)\n",
    "        return testx, testy\n",
    "    elif subset== 'valid':\n",
    "        validx, validy = valid_set\n",
    "        validx = validx.astype(np.float32).reshape(validx.shape[0], 28, 28)\n",
    "        validy = validy.astype(np.uint8)\n",
    "        return validx, validy\n",
    "    else:\n",
    "        raise NotImplementedError('subset should be train or valid or test')\n",
    "\n",
    "# Load data\n",
    "train_data, train_label = load(DATA_DIR, 'train')\n",
    "valid_data, valid_label = load(DATA_DIR, 'valid')\n",
    "test_data, test_label = load(DATA_DIR, 'test')\n",
    "\n",
    "# concatenate train and valid data as train data\n",
    "train_data = np.concatenate((train_data, valid_data))\n",
    "train_label = np.concatenate((train_label, valid_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST 데이터 확인 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# size of MNIST\n",
    "print(train_data.shape)\n",
    "print(train_label.shape)\n",
    "print(test_data.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show data\n",
    "_, (ax1, ax2) = plt.subplots(1, 2)\n",
    "sample_data = train_data[100]\n",
    "ax1.imshow(sample_data, cmap=plt.cm.Greys);\n",
    "ax2.hist(sample_data, bins=20, range=[0, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import LSTMCell\n",
    "#from tf.nn.rnn_cell import LSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "num_epochs = 2\n",
    "batch_size = 100\n",
    "num_display = 100\n",
    "\n",
    "def RNN(X):\n",
    "    X = tf.unstack(X, num=28, axis = 1)\n",
    "    lstm = LSTMCell(128, forget_bias = 0.9)\n",
    "    outs, _ = tf.nn.static_rnn(lstm, X, dtype= tf.float32)\n",
    "    return outs[-1]\n",
    "\n",
    "# static_rnn, dynamic_rnn \n",
    "def get_model(X, by, is_reuse):\n",
    "#    X = tf.expand_dims(X, axis=3) # (None, 28, 28, 1)\n",
    "    outs = RNN(X)\n",
    "    \n",
    "    outs = tf.layers.dense(outs, 128)\n",
    "    outs = tf.nn.relu(outs)\n",
    "    outs = tf.layers.dense(outs, 10)\n",
    "    one_hot = tf.one_hot(by, 10)\n",
    "\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=outs, \n",
    "                                                      labels=one_hot)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    opt = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    \n",
    "    preds = tf.cast(tf.argmax(tf.nn.softmax(outs), axis=1), tf.int32)\n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(by, preds), tf.float32))\n",
    "    init = tf.global_variables_initializer()\n",
    "    return {\n",
    "        'loss': loss,\n",
    "        'opt': opt,\n",
    "        'preds': preds,\n",
    "        'acc': acc,\n",
    "        'init': init,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, 28, 28))\n",
    "by = tf.placeholder(tf.int32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 8338604032",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-06c0c2d9c5c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'init'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mind_epoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Current iteration {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind_epoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TM/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m   1549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m     \u001b[0;31m# NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_graph_context_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TM/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewSessionRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 8338604032"
     ]
    }
   ],
   "source": [
    "model = get_model(X, by, False)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(model['init'])\n",
    "    for ind_epoch in range(0, num_epochs):\n",
    "        print('Current iteration {}'.format(ind_epoch + 1))\n",
    "        \n",
    "        for ind_ in range(0, int(60000 / batch_size)):\n",
    "            batch_X = train_data[ind_*batch_size:(ind_+1)*batch_size]\n",
    "            batch_by = train_label[ind_*batch_size:(ind_+1)*batch_size]\n",
    "            _, cur_loss, cur_acc = sess.run(\n",
    "                [model['opt'], model['loss'], model['acc']],\n",
    "                feed_dict={X: batch_X, by: batch_by})\n",
    "            if ind_ % num_display == 0:\n",
    "                print('loss {0:.4f} acc {1:.4f}'.format(cur_loss, cur_acc))\n",
    "    model['saver'].save(sess, './ours.ckpt')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(X, by, True)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(model['init'])\n",
    "    model['saver'].restore(sess, './ours.ckpt')\n",
    "    \n",
    "    cur_acc_all = 0.0\n",
    "    cur_loss_all = 0.0\n",
    "    for ind_ in range(0, 10):\n",
    "        cur_loss, cur_acc = sess.run(\n",
    "                    [model['loss'], model['acc']],\n",
    "                    feed_dict={X: test_data[ind_*1000:(ind_+1)*1000], \n",
    "                               by: test_label[ind_*1000:(ind_+1)*1000]})\n",
    "        cur_loss_all += cur_loss\n",
    "        cur_acc_all += cur_acc\n",
    "    print('TEST: loss {0:.4f} acc {1:.4f}'.format(cur_loss_all / 10.0, \n",
    "                                                  cur_acc_all / 10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TM",
   "language": "python",
   "name": "tm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
